import nltk
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
###loading Dataset
df=pd.read_csv("comments.csv")
df.shape

df.head(20)

df.describe()

df.isnull().sum()

rowSums = df.iloc[:,2:].sum(axis=1)
clean_comments_count = (rowSums==0).sum(axis=0)

print("Total number of comments = ",len(df))
print("Number of clean comments = ",clean_comments_count)
print("Number of comments with labels =",(len(df)-clean_comments_count))

df['text length'] = df['comment_text'].apply(len)
df.head()

import seaborn as sns
sns.distplot(a=df['text length'],bins=20)

###counting total number of comments of different categories
categories = list(df.columns.values)
categories = categories[2:8]
print(categories)
counts = []
for category in categories:
    counts.append((category, df[category].sum()))
df_stats = pd.DataFrame(counts, columns=['category', 'number of comments'])
df_stats

##ax= sns.barplot(categories, data_raw.iloc[:,2:].sum().values)
plt.bar(categories,df.iloc[:,2:8].sum().values,color='black')
plt.title("count of various categories of comments in dataset")
plt.xlabel("categories")
plt.ylabel('count')


from wordcloud import WordCloud,STOPWORDS
plt.figure(figsize=(20,20))
#toxic
sub_set = df[df.toxic==1]
text = sub_set.comment_text.values
cloud_toxic = WordCloud(
                          stopwords=STOPWORDS,
                          background_color='black',
                          collocations=False,
                          width=250,
                          height=180
                         ).generate(" ".join(text))

plt.subplot(1, 3, 1)
plt.axis('off')
plt.title(" Glipse of Toxic Comments",fontsize=40)
plt.imshow(cloud_toxic)

plt.figure(figsize=(20,20))

# insult
sub_set = df[df.insult==1]
text = sub_set.comment_text.values
cloud_toxic = WordCloud(
                          stopwords=STOPWORDS,
                          background_color='black',
                          collocations=False,
                          width=250,
                          height=180
                         ).generate(" ".join(text))

plt.subplot(1, 3, 2)
plt.axis('off')
#plt.title("insult",fontsize=40)
plt.imshow(cloud_toxic)

#threat
sub_set = df[df.threat==1]
text = sub_set.comment_text.values
cloud_toxic = WordCloud(
                          stopwords=STOPWORDS,
                          background_color='black',
                          collocations=False,
                          width=250,
                          height=180
                         ).generate(" ".join(text))


plt.subplot(1, 3, 3)
plt.axis('off')
#plt.title("threat",fontsize=40)
plt.imshow(cloud_toxic)

#cleaning of dataset
def remove_whitespace(text):
    return  " ".join(text.split())

df['comment_text']=df['comment_text'].apply(remove_whitespace)
print(df['comment_text'])


##converting the comment_text in lower case
df['comment_text']=df['comment_text'].str.lower()
df.head()

#list of stopwords
import nltk
nltk.download()
from nltk.corpus import stopwords
print(stopwords.words('english'))


##pre-processing of dataset
##tokenisation
from nltk import word_tokenize
print(df['comment_text'])
df['comment_text']=df['comment_text'].apply(lambda X: word_tokenize(X))
#df.head()

df.head()

#removing stopwords
en_stopwords = stopwords.words('english')

def remove_stopwords(text):
    result = []
    for token in text:
        if token not in en_stopwords:
            result.append(token)
            
    return result
    

df['comment_text'] = df['comment_text'].apply(remove_stopwords)
#df.head()

df.head()


df['comment_text']


#performing lemmatization
from nltk.stem import WordNetLemmatizer
from nltk import word_tokenize,pos_tag

def lemmatization(text):
    
    result=[]
    wordnet = WordNetLemmatizer()
    for token,tag in pos_tag(text):
        pos=tag[0].lower()
        
        if pos not in ['a', 'r', 'n', 'v']:
            pos='n'
            
        result.append(wordnet.lemmatize(token,pos))
    
    return result

df['comment_text']=df['comment_text'].apply(lemmatization)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report,confusion_matrix

tfidf_vec = TfidfVectorizer(max_df=0.7,stop_words='english',lowercase=False)

print(tfidf_vec)

TfidfVectorizer(lowercase=False, max_df=0.7, stop_words='english')

df=pd.read_csv("comments.csv")


##logistic regression applied on each category level separately and probability is predicted.
X = df['comment_text']
y = df['toxic']
print(" Toxic Comments analysis")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
X_train_vec = tfidf_vec.fit_transform(X_train)
X_test_vec = tfidf_vec.transform(X_test)

log_toxic = LogisticRegression()
log_toxic.fit(X_train_vec,y_train)

predictions = log_toxic.predict(X_test_vec)
print("confusion Matrix")
print(confusion_matrix(y_test,predictions))
print("Classification Report")
print(classification_report(y_test,predictions))
prob_stoxic = log_toxic.predict_proba(X_test_vec)


X = df['comment_text']
y = df['severe_toxic']
print(" Severe_Toxic Comments analysis")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)
X_train_vec = tfidf_vec.fit_transform(X_train)
X_test_vec = tfidf_vec.transform(X_test)

log_severe_toxic = LogisticRegression()
log_severe_toxic.fit(X_train_vec,y_train)

predictions = log_toxic.predict(X_test_vec)
print("confusion Matrix")
print(confusion_matrix(y_test,predictions))
print("Classification Report")
print(classification_report(y_test,predictions))
prob_severe_toxic = log_severe_toxic.predict_proba(X_test_vec)


X = df['comment_text']
y = df['obscene']
print(" obscene comments analysis")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
X_train_vec = tfidf_vec.fit_transform(X_train)
X_test_vec = tfidf_vec.transform(X_test)

log_obscene_toxic = LogisticRegression()
log_obscene_toxic.fit(X_train_vec,y_train)

predictions = log_toxic.predict(X_test_vec)
print("confusion Matrix")
print(confusion_matrix(y_test,predictions))
print("Classification Report")
print(classification_report(y_test,predictions))
prob_obscene_toxic = log_obscene_toxic.predict_proba(X_test_vec)



X = df['comment_text']
y = df['threat']
print(" threat comments analysis")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
X_train_vec = tfidf_vec.fit_transform(X_train)
X_test_vec = tfidf_vec.transform(X_test)

log_threat_toxic = LogisticRegression()
log_threat_toxic.fit(X_train_vec,y_train)

predictions = log_toxic.predict(X_test_vec)
print("confusion Matrix")
print(confusion_matrix(y_test,predictions))
print("Classification Report")
print(classification_report(y_test,predictions))
prob_threat_toxic = log_threat_toxic.predict_proba(X_test_vec)



X = df['comment_text']
y = df['insult']
print(" insult comments analysis")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
X_train_vec = tfidf_vec.fit_transform(X_train)
X_test_vec = tfidf_vec.transform(X_test)

log_insult_toxic = LogisticRegression()
log_insult_toxic.fit(X_train_vec,y_train)

predictions = log_toxic.predict(X_test_vec)
print("confusion Matrix")
print(confusion_matrix(y_test,predictions))
print("Classification Report")
print(classification_report(y_test,predictions))
prob_insult_toxic = log_insult_toxic.predict_proba(X_test_vec)



X = df['comment_text']
y = df['identity_hate']
print(" insult comments analysis")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
X_train_vec = tfidf_vec.fit_transform(X_train)
X_test_vec = tfidf_vec.transform(X_test)

log_identity_hate_toxic = LogisticRegression()
log_identity_hate_toxic.fit(X_train_vec,y_train)

predictions = log_toxic.predict(X_test_vec)
print("confusion Matrix")
print(confusion_matrix(y_test,predictions))
print("Classification Report")
print(classification_report(y_test,predictions))
prob_identity_hate_toxic = log_identity_hate_toxic.predict_proba(X_test_vec)



#storing the predicted probability in  a dataframe
d1 = pd.DataFrame(prob_stoxic[:,1],columns={'toxic'})
d2 = pd.DataFrame(prob_severe_toxic[:,1],columns={'severe_toxic'})
d3 = pd.DataFrame(prob_obscene_toxic[:,1],columns={'obscene'})
d4 = pd.DataFrame(prob_threat_toxic[:,1],columns={'threat'})
d5 = pd.DataFrame(prob_insult_toxic[:,1],columns={'insult'})
d6 = pd.DataFrame(prob_identity_hate_toxic[:,1],columns={'identity_hate'})



predicted_final_dataset = pd.concat([df['comment_text'],d1,d2,d3,d4,d5,d6],axis=1)


predicted_final_dataset.head(10)


predicted_final_dataset.describe()

